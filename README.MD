# 个人学习强化学习复现代码（仅供参考）
|论文|pytorch代码|实现情况|
|:-:|:-:|:-:|
|[Deep Q-Network (DQN)](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)|DQN|✅|
|[Double DQN](https://arxiv.org/pdf/1509.06461.pdf)|Double-DQN|❌|
|[Dueling DQN](https://arxiv.org/pdf/1511.06581.pdf)|Dueling-DQN|❌|
|[Categorical DQN (C51)](https://arxiv.org/pdf/1707.06887.pdf)|C51|❌|
|[Rainbow DQN (Rainbow)](https://arxiv.org/pdf/1710.02298.pdf)|Rainbow|❌|
|[Quantile Regression DQN (QRDQN)](https://arxiv.org/pdf/1710.10044.pdf)|QRDQN|❌|
|[Implicit Quantile Network (IQN)](https://arxiv.org/pdf/1806.06923.pdf)|IQN|❌|
|[Fully-parameterized Quantile Function (FQF)](https://arxiv.org/pdf/1911.02140.pdf)|FQF|❌|
|[Policy Gradient (PG)](Policy Gradient (PG))|PG|❌|
|[Natural Policy Gradient (NPG)](https://proceedings.neurips.cc/paper/2001/file/4b86abe48d358ecf194c56c69108433e-Paper.pdf)|NPG|❌|
|[Advantage Actor-Critic (A2C)](https://openai.com/blog/baselines-acktr-a2c/)|A2C|❌|
|[Trust Region Policy Optimization (TRPO)](https://arxiv.org/pdf/1502.05477.pdf)|TRPO|❌|
|[Proximal Policy Optimization (PPO)](https://arxiv.org/pdf/1707.06347.pdf)|PPO|❌|
|[Deep Deterministic Policy Gradient (DDPG)](https://arxiv.org/pdf/1509.02971.pdf)|DDPG|❌|
|[Twin Delayed DDPG (TD3)](https://arxiv.org/pdf/1802.09477.pdf)|TD3|❌|
|[Soft Actor-Critic (SAC)](https://arxiv.org/pdf/1812.05905.pdf)|SAC|❌|
|[Discrete Soft Actor-Critic (SAC-Discrete)](https://arxiv.org/pdf/1910.07207.pdf)|SAC-Discrete|❌|
|Vanilla Imitation Learning|VIL|❌|
|[Batch-Constrained deep Q-Learning (BCQ)](https://arxiv.org/pdf/1812.02900.pdf)|BCQ|❌|
|[Conservative Q-Learning (CQL)](https://arxiv.org/pdf/2006.04779.pdf)|CQL|❌|
|[Discrete Batch-Constrained deep Q-Learning (BCQ-Discrete)](https://arxiv.org/pdf/1910.01708.pdf)|BCQ-Discrete|❌|
|[Discrete Conservative Q-Learning (CQL-Discrete)](https://arxiv.org/pdf/2006.04779.pdf)|CQL-Discrete|❌|
|[Discrete Critic Regularized Regression (CRR-Discrete)](https://arxiv.org/pdf/2006.15134.pdf)|CRR-Discrete|❌|
|[Generative Adversarial Imitation Learning (GAIL)](https://arxiv.org/pdf/1606.03476.pdf)|GAIL|❌|
|[Prioritized Experience Replay (PER)](https://arxiv.org/pdf/1511.05952.pdf)|PER|❌|
|[Generalized Advantage Estimator (GAE)](https://arxiv.org/pdf/1506.02438.pdf)|GAE|❌|
|[Posterior Sampling Reinforcement Learning (PSRL)](https://www.ece.uvic.ca/~bctill/papers/learning/Strens_2000.pdf)|PSRL|❌|
|[Intrinsic Curiosity Module (ICM)](https://arxiv.org/pdf/1705.05363.pdf)|ICM|❌|
