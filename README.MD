# 个人学习强化学习复现代码（仅供参考）
|论文|pytorch代码|实现情况|
|:-:|:-:|:-:|
|[Deep Q-Network (DQN)](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)|DQN|✅|
|[Double DQN](https://arxiv.org/pdf/1509.06461.pdf)|Double-DQN|❌|
|[Randomized Ensembled Double Q-Learning(REDQ)](https://openreview.net/pdf?id=AY8zfZm0tDd)|REDQ|❌|
|[Recurrent experience replay in distributed reinforcement learning(R2D2)](https://openreview.net/pdf?id=r1lyTjAqYX)|R2D2|❌|
|[Soft Q Learning (SQL)](https://arxiv.org/pdf/1702.08165.pdf)|SQL|❌|
|[Dueling Network Architectures for Deep Reinforcement Learning(D3QN)](http://arxiv.org/pdf/1511.06581)|REDQ|❌|
|[Dueling DQN](https://arxiv.org/pdf/1511.06581.pdf)|Dueling-DQN|❌|
|[Categorical DQN (C51)](https://arxiv.org/pdf/1707.06887.pdf)|C51|❌|
|[Rainbow DQN (Rainbow)](https://arxiv.org/pdf/1710.02298.pdf)|Rainbow|❌|
|[Quantile Regression DQN (QRDQN)](https://arxiv.org/pdf/1710.10044.pdf)|QRDQN|❌|
|[Implicit Quantile Network (IQN)](https://arxiv.org/pdf/1806.06923.pdf)|IQN|❌|
|[Fully-parameterized Quantile Function (FQF)](https://arxiv.org/pdf/1911.02140.pdf)|FQF|❌|
|[Policy Gradient (PG)](https://homes.cs.washington.edu/~todorov/courses/amath579/reading/PolicyGradient.pdf)|PG|❌|
|[Natural Policy Gradient (NPG)](https://proceedings.neurips.cc/paper/2001/file/4b86abe48d358ecf194c56c69108433e-Paper.pdf)|NPG|❌|
|[Advantage Actor-Critic (A2C)](https://openai.com/blog/baselines-acktr-a2c/)|A2C|❌|
|[Sample Efficient Actor-Critic with Experience Replay (ACER)](https://arxiv.org/abs/1611.01224)|ACER|❌|
|[Trust Region Policy Optimization (TRPO)](https://arxiv.org/pdf/1502.05477.pdf)|TRPO|❌|
|[Proximal Policy Optimization (PPO)](https://arxiv.org/pdf/1707.06347.pdf)|PPO|❌|
|[Deep Deterministic Policy Gradient (DDPG)](https://arxiv.org/pdf/1509.02971.pdf)|DDPG|❌|
|[Twin Delayed DDPG (TD3)](https://arxiv.org/pdf/1802.09477.pdf)|TD3|❌|
|[Soft Actor-Critic (SAC)](https://arxiv.org/pdf/1812.05905.pdf)|SAC|❌|
|[IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures (IMPALA)](https://arxiv.org/abs/1802.01561)|IMPALA|❌|
|[Discrete Soft Actor-Critic (SAC-Discrete)](https://arxiv.org/pdf/1910.07207.pdf)|SAC-Discrete|❌|
|[Batch-Constrained deep Q-Learning (BCQ)](https://arxiv.org/pdf/1812.02900.pdf)|BCQ|❌|
|[Phasic Policy Gradient (PPG)](https://arxiv.org/abs/2009.04416)|PPG|❌|
|[A Minimalist Approach to Offline Reinforcement Learning (TD3BC)](https://arxiv.org/abs/2106.06860)|TD3BC|❌|
|[Distributed Distributional Deterministic Policy Gradients (D4PG)](https://arxiv.org/abs/1804.08617v1)|D4PG|❌|
|[Continuous control with deep reinforcement learning (DDPG)](https://arxiv.org/abs/1509.02971)|DDPG|❌|
|[Conservative Q-Learning (CQL)](https://arxiv.org/pdf/2006.04779.pdf)|CQL|❌|
|[Discrete Batch-Constrained deep Q-Learning (BCQ-Discrete)](https://arxiv.org/pdf/1910.01708.pdf)|BCQ-Discrete|❌|
|[Discrete Conservative Q-Learning (CQL-Discrete)](https://arxiv.org/pdf/2006.04779.pdf)|CQL-Discrete|❌|
|[Discrete Critic Regularized Regression (CRR-Discrete)](https://arxiv.org/pdf/2006.15134.pdf)|CRR-Discrete|❌|
|[Generative Adversarial Imitation Learning (GAIL)](https://arxiv.org/pdf/1606.03476.pdf)|GAIL|❌|
|[Prioritized Experience Replay (PER)](https://arxiv.org/pdf/1511.05952.pdf)|PER|❌|
|[Generalized Advantage Estimator (GAE)](https://arxiv.org/pdf/1506.02438.pdf)|GAE|❌|
|[Posterior Sampling Reinforcement Learning (PSRL)](https://www.ece.uvic.ca/~bctill/papers/learning/Strens_2000.pdf)|PSRL|❌|
|[Intrinsic Curiosity Module (ICM)](https://arxiv.org/pdf/1705.05363.pdf)|ICM|❌|
|[QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning (QMIX)](https://arxiv.org/abs/1803.11485)|QMIX|❌|
|[Phasic Policy Gradient (COMA)](https://proceedings.neurips.cc/paper/2016/file/c7635bfd99248a2cdef8249ef7bfbef4-Paper.pdf)|COMA|❌|
|[Weighted QMIX: Expanding Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning (WQMIX)](https://arxiv.org/abs/2006.10800)|WQMIX|❌|
|[Multi-Agent Collaboration via Reward Attribution Decomposition (CollaQ)](https://arxiv.org/abs/2010.08531)|CollaQ|❌|
|[Learning Attentional Communication for Multi-Agent Cooperation (ATOC)](https://arxiv.org/abs/1805.07733)|ATOC|❌|
|[Exploration by Random Network Distillation (RND)](https://arxiv.org/abs/1810.12894v1)|RND|❌|
|[Hindsight Experience Replay (HER)](https://arxiv.org/abs/1707.01495)|HER|❌|
|[Deep Q-learning from Demonstrations (DQfD)](https://arxiv.org/abs/1704.03732)|DQfD|❌|
|[SQIL: Imitation Learning via Reinforcement Learning with Sparse Rewards (SQIL)](https://arxiv.org/abs/1905.11108)|SQIL|❌|
|[Generative Adversarial Imitation Learning (GAIL)](https://arxiv.org/abs/1606.03476)|GAIL|❌|
|[Extrapolating Beyond Suboptimal Demonstrations via Inverse Reinforcement Learning from Observations (TREX)](https://arxiv.org/abs/1904.06387)|TREX|❌|
|[Curiosity-driven Exploration by Self-supervised Prediction (ICM)](http://proceedings.mlr.press/v70/pathak17a/pathak17a.pdf)|ICM|❌|
|[Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization(GCL)](https://arxiv.org/abs/1603.00448)|GCL|❌|
|[Making Efficient Use of Demonstrations to Solve Hard Exploration Problems(R2D3)](https://arxiv.org/abs/1909.01387)|R2D3|❌|
|[When to Trust Your Model: Model-Based Policy Optimization(MBPO)](https://arxiv.org/abs/1906.08253)|MBPO|❌|
|[Value Prediction Network(VPN)](https://arxiv.org/abs/1707.03497)|VPN|❌|
|[Monte Carlo Tree Search(MCTS)](https://digitalcommons.morris.umn.edu/cgi/viewcontent.cgi?referer=&httpsredir=1&article=1028&context=horizons)|MCTS|❌|
|[Value-Decomposition Networks For Cooperative Multi-Agent Learning(VDN)](https://arxiv.org/abs/1707.03497)|VDN|❌|
|[QTRAN: Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement learning(QTRAN)]()|QTRAN|❌|
|[MultiAgent Cooperation and Competition with Deep Reinforcement Learning(IQL)](https://arxiv.org/abs/1707.03497)|IQL|❌|
|[Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments(MADDPG)](https://openai.com/blog/learning-to-cooperate-compete-and-communicate/)|MADDPG|❌|
|[The Surprising Effectiveness of MAPPO in Cooperative, Multi-Agent Games(MAPPO)](https://arxiv.org/abs/2103.01955)|MAPPO|❌|
|[MatD3: A Database and Online Presentation Package for Research Data Supporting Materials Discovery, Design, and Dissemination(MATD3)](https://arxiv.org/abs/2001.02135)|MATD3|❌|
|[Mastering the game of Go with deep neural networks and tree search(AlphaGo)](https://www.nature.com/articles/nature16961)|AlphaGo|❌|
|[Mastering the game of Go without human knowledge(AlphaGoZero)](https://www.nature.com/articles/nature24270.)|AlphaGoZero|❌|
|...|...|...|
