# 个人学习强化学习复现代码（仅供参考）
|论文|pytorch代码|实现情况|提出时间|
|:-:|:-:|:-:|:-:|
|[Policy Gradient (PG)](https://homes.cs.washington.edu/~todorov/courses/amath579/reading/PolicyGradient.pdf)|PG|❌|1999|
|[Natural Policy Gradient (NPG)](https://proceedings.neurips.cc/paper/2001/file/4b86abe48d358ecf194c56c69108433e-Paper.pdf)|NPG|❌|2001|
|[Trust Region Policy Optimization (TRPO)](https://arxiv.org/pdf/1502.05477.pdf)|TRPO|❌|2015|
|[Continuous control with deep reinforcement learning (DDPG)](https://arxiv.org/abs/1509.02971)|DDPG|❌|2015|
|[Deep Deterministic Policy Gradient (DDPG)](https://arxiv.org/pdf/1509.02971.pdf)|DDPG|❌|2015|
|[Deep Q-Network (DQN)](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)|DQN|✅|2015|
|[Monte Carlo Tree Search(MCTS)](https://digitalcommons.morris.umn.edu/cgi/viewcontent.cgi?referer=&httpsredir=1&article=1028&context=horizons)|MCTS|❌|2015|
|[Prioritized Experience Replay (PER)](https://arxiv.org/pdf/1511.05952.pdf)|PER|❌|2015|
|[Generalized Advantage Estimator (GAE)](https://arxiv.org/pdf/1506.02438.pdf)|GAE|❌|2015|
|[Double DQN](https://arxiv.org/pdf/1509.06461.pdf)|Double-DQN|❌|2016|
|[Dueling Network Architectures for Deep Reinforcement Learning(D3QN)](http://arxiv.org/pdf/1511.06581)|REDQ|❌|2016|
|[Dueling DQN](https://arxiv.org/pdf/1511.06581.pdf)|Dueling-DQN|❌|2016|
|[Advantage Actor-Critic (A2C)](https://arxiv.org/abs/1602.01783)|A2C|❌|2016|
|[Asynchronous Methods for Deep Reinforcement Learning (A3C)](https://arxiv.org/abs/1602.01783)|A3C|❌|2016|
|[Sample Efficient Actor-Critic with Experience Replay (ACER)](https://arxiv.org/abs/1611.01224)|ACER|❌|2016|
|[Generative Adversarial Imitation Learning (GAIL)](https://arxiv.org/pdf/1606.03476.pdf)|GAIL|❌|2016|
|[Phasic Policy Gradient (COMA)](https://proceedings.neurips.cc/paper/2016/file/c7635bfd99248a2cdef8249ef7bfbef4-Paper.pdf)|COMA|❌|2016|
|[Generative Adversarial Imitation Learning (GAIL)](https://arxiv.org/abs/1606.03476)|GAIL|❌|2016|
|[Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization(GCL)](https://arxiv.org/abs/1603.00448)|GCL|❌|2016|
|[Mastering the game of Go with deep neural networks and tree search(AlphaGo)](https://www.nature.com/articles/nature16961)|AlphaGo|❌|2016|
|[Mastering the game of Go without human knowledge(AlphaGoZero)](https://www.nature.com/articles/nature24270.)|AlphaGoZero|❌|2017|
|[Hindsight Experience Replay (HER)](https://arxiv.org/abs/1707.01495)|HER|❌|2017|
|[Deep Q-learning from Demonstrations (DQfD)](https://arxiv.org/abs/1704.03732)|DQfD|❌|2017|
|[Curiosity-driven Exploration by Self-supervised Prediction (ICM)](http://proceedings.mlr.press/v70/pathak17a/pathak17a.pdf)|ICM|❌|2017|
|[Value Prediction Network(VPN)](https://arxiv.org/abs/1707.03497)|VPN|❌|2017|
|[Value-Decomposition Networks For Cooperative Multi-Agent Learning(VDN)](https://arxiv.org/abs/1706.05296)|VDN|❌|2017|
|[MultiAgent Cooperation and Competition with Deep Reinforcement Learning(IQL)](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0172395)|IQL|❌|2017|
|[Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments(MADDPG)](https://openai.com/blog/learning-to-cooperate-compete-and-communicate/)|MADDPG|❌|2017|
|[Soft Q Learning (SQL)](https://arxiv.org/pdf/1702.08165.pdf)|SQL|❌|2017|
|[Categorical DQN (C51)](https://arxiv.org/pdf/1707.06887.pdf)|C51|❌|2017|
|[Proximal Policy Optimization (PPO)](https://arxiv.org/pdf/1707.06347.pdf)|PPO|❌|2017|
|[Intrinsic Curiosity Module (ICM)](https://arxiv.org/pdf/1705.05363.pdf)|ICM|❌|2017|
|[QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning (QMIX)](https://arxiv.org/abs/1803.11485)|QMIX|❌|2018|
|[Learning Attentional Communication for Multi-Agent Cooperation (ATOC)](https://arxiv.org/abs/1805.07733)|ATOC|❌|2018|
|[Exploration by Random Network Distillation (RND)](https://arxiv.org/abs/1810.12894v1)|RND|❌|2018|
|[Recurrent experience replay in distributed reinforcement learning(R2D2)](https://openreview.net/pdf?id=r1lyTjAqYX)|R2D2|❌|2018|
|[Distributed Distributional Deterministic Policy Gradients (D4PG)](https://arxiv.org/abs/1804.08617v1)|D4PG|❌|2018|
|[Rainbow DQN (Rainbow)](https://arxiv.org/pdf/1710.02298.pdf)|Rainbow|❌|2018|
|[Quantile Regression DQN (QRDQN)](https://arxiv.org/pdf/1710.10044.pdf)|QRDQN|❌|2018|
|[Implicit Quantile Network (IQN)](https://arxiv.org/pdf/1806.06923.pdf)|IQN|❌|2018|
|[Twin Delayed DDPG (TD3)](https://arxiv.org/pdf/1802.09477.pdf)|TD3|❌|2018|
|[Soft Actor-Critic (SAC)](https://arxiv.org/pdf/1812.05905.pdf)|SAC|❌|2018|
|[IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures (IMPALA)](https://arxiv.org/abs/1802.01561)|IMPALA|❌|2018|
|[Discrete Soft Actor-Critic (SAC-Discrete)](https://arxiv.org/pdf/1910.07207.pdf)|SAC-Discrete|❌|2019|
|[Discrete Batch-Constrained deep Q-Learning (BCQ-Discrete)](https://arxiv.org/pdf/1910.01708.pdf)|BCQ-Discrete|❌|2019|
|[Batch-Constrained deep Q-Learning (BCQ)](https://arxiv.org/pdf/1812.02900.pdf)|BCQ|❌|2019|
|[Fully-parameterized Quantile Function (FQF)](https://arxiv.org/pdf/1911.02140.pdf)|FQF|❌|2019|
|[SQIL: Imitation Learning via Reinforcement Learning with Sparse Rewards (SQIL)](https://arxiv.org/abs/1905.11108)|SQIL|❌|2019|
|[Extrapolating Beyond Suboptimal Demonstrations via Inverse Reinforcement Learning from Observations (TREX)](https://arxiv.org/abs/1904.06387)|TREX|❌|2019|
|[Making Efficient Use of Demonstrations to Solve Hard Exploration Problems(R2D3)](https://arxiv.org/abs/1909.01387)|R2D3|❌|2019|
|[When to Trust Your Model: Model-Based Policy Optimization(MBPO)](https://arxiv.org/abs/1906.08253)|MBPO|❌|2019|
|[QTRAN: Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement learning(QTRAN)](https://proceedings.mlr.press/v97/son19a.html)|QTRAN|❌|2019|
|[MatD3: A Database and Online Presentation Package for Research Data Supporting Materials Discovery, Design, and Dissemination(MATD3)](https://arxiv.org/abs/2001.02135)|MATD3|❌|2019|
|[Weighted QMIX: Expanding Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning (WQMIX)](https://arxiv.org/abs/2006.10800)|WQMIX|❌|2020|
|[Multi-Agent Collaboration via Reward Attribution Decomposition (CollaQ)](https://arxiv.org/abs/2010.08531)|CollaQ|❌|2020|
|[Discrete Conservative Q-Learning (CQL-Discrete)](https://arxiv.org/pdf/2006.04779.pdf)|CQL-Discrete|❌|2020|
|[Discrete Critic Regularized Regression (CRR-Discrete)](https://arxiv.org/pdf/2006.15134.pdf)|CRR-Discrete|❌|2020|
|[Posterior Sampling Reinforcement Learning (PSRL)](https://www.ece.uvic.ca/~bctill/papers/learning/Strens_2000.pdf)|PSRL|❌|2000|
|[Conservative Q-Learning (CQL)](https://arxiv.org/pdf/2006.04779.pdf)|CQL|❌|2020|
|[Phasic Policy Gradient (PPG)](https://arxiv.org/abs/2009.04416)|PPG|❌|2021|
|[A Minimalist Approach to Offline Reinforcement Learning (TD3BC)](https://arxiv.org/abs/2106.06860)|TD3BC|❌|2021|
|[Randomized Ensembled Double Q-Learning(REDQ)](https://openreview.net/pdf?id=AY8zfZm0tDd)|REDQ|❌|2021|
|[The Surprising Effectiveness of MAPPO in Cooperative, Multi-Agent Games(MAPPO)](https://arxiv.org/abs/2103.01955)|MAPPO|❌|2021|
|...|...|...|
